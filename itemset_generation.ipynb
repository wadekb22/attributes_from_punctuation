{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our dictionary for punctuation to word conversion\n",
    "\n",
    "marks = ['.',',',':',';','?','!','-','(',')','\"',\"'\",'~']\n",
    "marks_to_wrds = {'.':'prd', ',':'cmma', ':':'coln', ';':'smicln', '?':'qstn', '!':'xclm', '-':'dsh', '(':'lftparen', ')':'rghtparen', '\"':'dblqt', \"'\":'snglqt', '~':'elpss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\n",
    "    \"the\", \"a\", \"an\"\n",
    "]\n",
    "\n",
    "pronouns = [\n",
    "    \"that\", \"this\", \"those\", \"these\", \"i\", \"we\", \"me\", \"my\", \"us\", \"ours\", \"you\", \"yours\", \n",
    "    \"he\", \"she\", \"it\", \"him\", \"her\", \"his\", \"hers\", \"its\", \"they\", \"them\", \"their\", \"here\", \"there\", \n",
    "    \"who\", \"whose\", \"whom\"\n",
    "]\n",
    "\n",
    "prepositions = [\n",
    "    \"to\", \"of\", \"in\", \"for\", \"on\", \"with\", \"at\", \"by\", \"from\", \"up\", \"about\", \"into\", \"after\", \"over\"\n",
    "]\n",
    "\n",
    "conjunctions = [\n",
    "    \"as\", \"but\", \"or\", \"and\", \"so\"\n",
    "]\n",
    "\n",
    "filler_words = articles + pronouns + prepositions + conjunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('stats_clean.csv', error_bad_lines = False, encoding = 'latin1')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ellipses are replaced\n",
      "urls removed\n",
      "blank chars removed\n",
      "changed to lowercase\n"
     ]
    }
   ],
   "source": [
    "## Data cleaning\n",
    "# Replace '...' with '~'\n",
    "data['text'] = data['text'].str.replace(r\"\\.\\.\\.\", \"~\")\n",
    "print(\"ellipses are replaced\")\n",
    "\n",
    "# Remove URLs\n",
    "data['text'] = data['text'].str.replace(r\"\\[.*?\\]\\(https?:\\/\\/.*?\\)\",\"\")\n",
    "print(\"urls removed\")\n",
    "\n",
    "# Remove blank characters\n",
    "data['text'] = data['text'].str.replace(r\"&#x200B;\",\"\")\n",
    "print(\"blank chars removed\")\n",
    "\n",
    "# to lowercase\n",
    "data['text'] = data['text'].str.lower()\n",
    "print(\"changed to lowercase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 24.5, 43.5, 55.5, 74.5, 100]\n",
    "gen_labels = [\"GenZ\", \"GenY\", \"GenX\", \"BabyBoomers\", \"Traditionalists\"]\n",
    "data['age'] = pd.cut(data['age'], bins=bins, labels=gen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create even sample across gender\n",
    "\n",
    "#BBs and trads too small a set\n",
    "data = data[(data['age']!='BabyBoomers') & (data['age']!='Traditionalists')]\n",
    "\n",
    "samps = []\n",
    "for a in data.age.unique():\n",
    "    m = data[(data['age']==a) & (data['gender']=='Male')]\n",
    "    m_size = len(m)\n",
    "    samps.append(m)\n",
    "    f = data[(data['age']==a) & (data['gender']=='Female')].sample(n=m_size, random_state=1)\n",
    "    samps.append(f)\n",
    "even_data = pd.concat(samps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = even_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert text to itemset data\n",
    "# Concatenate all the comments into one long string.\n",
    "\n",
    "# data to use\n",
    "df = data\n",
    "\n",
    "punc_pat = df['text'].str.extractall(r\"(?P<first>[\\w']+)\\s+(?P<second>[\\w']+)\\s+(?P<third>[\\w']+)\\s+(?P<fourth>[\\w']+)\\b\\s?(?P<punc>[.,:;?!\\-()\\\"~])\")\n",
    "itemset = punc_pat.join(data[['gender','age']].reindex(punc_pat.index,level=0))\n",
    "itemset['punc'] = itemset['punc'].map(marks_to_wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing itemsets\\freqitems_GenY_prd.csv\n",
      "Subset word count reduced to 3986\n",
      "Processing itemsets\\freqitems_GenX_prd.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenZ_prd.csv\n",
      "Subset word count reduced to 4508\n",
      "Processing itemsets\\freqitems_GenY_cmma.csv\n",
      "Subset word count reduced to 8853\n",
      "Processing itemsets\\freqitems_GenX_cmma.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenZ_cmma.csv\n",
      "Subset word count reduced to 8462\n",
      "Processing itemsets\\freqitems_GenY_coln.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenX_coln.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenZ_coln.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenY_smicln.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenX_smicln.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenZ_smicln.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenY_qstn.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenX_qstn.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenZ_qstn.csv\n",
      "Subset word count reduced to 35129\n",
      "Processing itemsets\\freqitems_GenY_xclm.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenX_xclm.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenZ_xclm.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenY_dsh.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenX_dsh.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenZ_dsh.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenY_lftparen.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenX_lftparen.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenZ_lftparen.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenY_rghtparen.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenX_rghtparen.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenZ_rghtparen.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenY_dblqt.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenX_dblqt.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenZ_dblqt.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenY_snglqt.csv\n",
      "Processing itemsets\\freqitems_GenX_snglqt.csv\n",
      "Processing itemsets\\freqitems_GenZ_snglqt.csv\n",
      "Processing itemsets\\freqitems_GenY_elpss.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenX_elpss.csv\n",
      "Subset word count is fine, proceeding\n",
      "Processing itemsets\\freqitems_GenZ_elpss.csv\n",
      "Subset word count is fine, proceeding\n"
     ]
    }
   ],
   "source": [
    "## Main methods\n",
    "\n",
    "trait_name = 'age'\n",
    "trait_itr = itemset.age.unique()\n",
    "sup = 0.002\n",
    "\n",
    "for punc in marks_to_wrds.values():\n",
    "    for trait in trait_itr:\n",
    "        \n",
    "        print(f\"Processing itemsets\\\\freqitems_{trait}_{punc}.csv\")\n",
    "        \n",
    "        subset = itemset[(itemset[trait_name]==trait) & (itemset['punc']==punc)]\n",
    "\n",
    "        subset = subset[['first','second','third','fourth']].values.tolist()\n",
    "        subset = [[i for i in sub if i not in filler_words] for sub in subset]\n",
    "        \n",
    "        subset = reduce_words(subset)\n",
    "        \n",
    "        out = get_fp(subset, sup)\n",
    "        \n",
    "        out.to_csv(f\"itemsets\\\\freqitems_{trait}_{punc}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fp(transactions, min_sup = 0.05):\n",
    "    \n",
    "    # Put the transactions into dataframe form, this allows us to perform FPgrowth on the resultant dataframe.\n",
    "    te = TransactionEncoder()\n",
    "    te_transformed = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_transformed, columns = te.columns_)\n",
    "\n",
    "    # Apply FPgrowth to the transactional dataframe.\n",
    "    freq_pats = fpgrowth(df, min_support = min_sup, use_colnames = True)\n",
    "    return freq_pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_words(subset, max_cells=6500000000):\n",
    "    \n",
    "    try:\n",
    "        new_size = int(max_cells/len(subset))\n",
    "    except ZeroDivisionError:\n",
    "        return subset\n",
    "        \n",
    "    oneline = [j for i in subset for j in i]\n",
    "    counted = Counter(oneline)\n",
    "    ordered = [value for value, count in counted.most_common()]\n",
    "\n",
    "    if len(ordered) <= new_size:\n",
    "        print(\"Subset word count is fine, proceeding\")\n",
    "        return subset\n",
    "    else:\n",
    "        print(f\"Subset word count reduced to {new_size}\")\n",
    "        freq_words = ordered[0:new_size]\n",
    "        reduced_subset = [[i for i in sub if i in freq_words] for sub in subset]\n",
    "        return reduced_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main method, Generate global frequent itemset (all punc)\n",
    "sup = 0.002\n",
    "\n",
    "print(f\"Processing itemsets\\\\freqitems_all.csv\")\n",
    "\n",
    "subset = itemset\n",
    "\n",
    "subset = subset[['first','second','third','fourth']].values.tolist()\n",
    "subset = [[i for i in sub if i not in filler_words] for sub in subset]\n",
    "\n",
    "subset = reduce_words(subset)\n",
    "\n",
    "out = get_fp(subset, sup)\n",
    "\n",
    "out.to_csv(f\"itemsets\\\\freqitems_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main method, Generate global frequent itemset\n",
    "sup = 0.002\n",
    "\n",
    "for punc in marks_to_wrds.values():\n",
    "\n",
    "    print(f\"Processing itemsets\\\\freqitems_all_{punc}.csv\")\n",
    "\n",
    "    subset = itemset[(itemset['punc']==punc)]\n",
    "\n",
    "    subset = subset[['first','second','third','fourth']].values.tolist()\n",
    "    subset = [[i for i in sub if i not in filler_words] for sub in subset]\n",
    "\n",
    "    subset = reduce_words(subset)\n",
    "\n",
    "    out = get_fp(subset, sup)\n",
    "\n",
    "    out.to_csv(f\"itemsets\\\\freqitems_all_{punc}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
