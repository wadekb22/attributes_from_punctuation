{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Username: dscc440bot\n",
    "Password: si.VbFxBHy559Jn\n",
    "\n",
    "Documentation:\n",
    "https://praw.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create read-only reddit instance\n",
    "import praw\n",
    "\n",
    "# This should be moved to a private praw.ini file at some point\n",
    "reddit = praw.Reddit(\n",
    " client_id=\"kuf-S39ElN0SpA\",\n",
    " client_secret=\"XVSM3GfyGRbhbdeXh-s3AtNqnP2FCg\",\n",
    " user_agent=\"pc:attributes.from.punctuation:v1.0.0 (by u/dscc440bot)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test code\n",
    "# Submission urls to pull top level comments from\n",
    "sample_submission_urls = [\n",
    "    \"https://www.reddit.com/r/AskReddit/comments/9phld1/serious_reddit_what_is_your_age_and_what_problem/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Test code\n",
    "for url in sample_submission_urls:\n",
    "    #generate list of top level comments from each submission\n",
    "    comments = reddit.submission(url=url).comments\n",
    "    for comment in comments:\n",
    "        if type(comment) is praw.models.MoreComments:  # Comments truncate into MoreComments object. This can be expanded manually\n",
    "            len(comment.comments())\n",
    "        else:\n",
    "            print(comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code\n",
    "# Subreddits to pull submissions from \n",
    "sample_subreddit_names = [\n",
    "    \"relationships\",\n",
    "    \"relationship_advice\",\n",
    "    \"RoastMe\",\n",
    "    \"RateMe\",\n",
    "    \"amiugly\",\n",
    "    \"AmItheAsshole\"\n",
    "]\n",
    "\n",
    "for sub_name in sample_subreddit_names:\n",
    "    print(\"###\" + sub_name)\n",
    "    #generate list of top posts from each subreddit\n",
    "    posts = reddit.subreddit(sub_name).top(\"all\")\n",
    "    for post in posts:\n",
    "            print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account not valid (probably suspended).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6f2221cfa37e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MoreComments object found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mentity_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_data_entry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1606780800\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ref date of 12/01/2020 midnight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mentity_row\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermalink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-8f4406209212>\u001b[0m in \u001b[0;36mgenerate_data_entry\u001b[1;34m(text_instance, current_utc)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m#If demographics are found, calculate puncuation statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mp_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_punctuation_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_instance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauthor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1596240000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1604188800\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#08/01/2020-11/01/2020\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp_stats\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-b9191553c23f>\u001b[0m in \u001b[0;36mcalculate_punctuation_stats\u001b[1;34m(redditor, start_time, end_time)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mindex_combos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mindex_strings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindex_combos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mfinal_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m     \u001b[1;31m#combos = list(itertools.product(marks_str, marks_str))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;31m#indices = list(range(marks_str))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "###Main method\n",
    "\n",
    "import csv\n",
    "\n",
    "#list of submissions with demographic-rich top comments\n",
    "submission_urls = [\n",
    "    \"https://www.reddit.com/r/AskReddit/comments/9phld1/serious_reddit_what_is_your_age_and_what_problem/\"\n",
    "]\n",
    "#list of subreddits with demographic-rich submissions\n",
    "subreddit_names = [\n",
    "   # \"relationships\",\n",
    "   # \"relationship_advice\",\n",
    "   # \"RoastMe\",\n",
    "   # \"RateMe\",\n",
    "   # \"amiugly\",\n",
    "   # \"AmItheAsshole\"\n",
    "]\n",
    "\n",
    "#initialize dataset (list of dictionaries)\n",
    "dataset = []\n",
    "\n",
    "#iterate through each list, generating data entries for each item and adding to dataset\n",
    "# first: submissions with demographic-rich top comments\n",
    "for url in submission_urls:\n",
    "    #generate list of top level comments from each submission\n",
    "    comments = reddit.submission(url=url).comments\n",
    "    for comment in comments:\n",
    "        if type(comment) is praw.models.MoreComments:  # Comments truncate into MoreComments object. This can be expanded manually\n",
    "            print(\"MoreComments object found\")\n",
    "        else:\n",
    "            entity_row = generate_data_entry(comment, 1606780800)  # ref date of 12/01/2020 midnight\n",
    "            if entity_row:\n",
    "                print(comment.permalink)\n",
    "                dataset.append(entity_row)\n",
    "                #print(entity_row)\n",
    "\n",
    "# second: subreddits with demographic-rich post titles\n",
    "for sub_name in subreddit_names:\n",
    "    #generate list of top posts from each subreddit\n",
    "    posts = reddit.subreddit(sub_name).top(\"all\")\n",
    "    for post in posts:\n",
    "        entity_row = generate_data_entry(post, 1606780800)  # ref date of 12/01/2020 midnight\n",
    "        if entity_row:\n",
    "            print(post.permalink)\n",
    "            dataset.append(entity_row)\n",
    "            #print(entity_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ffd015827b16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#save dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#code adapted from https://stackoverflow.com/questions/3086973/how-do-i-convert-this-list-of-dictionaries-to-a-csv-file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stats.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdict_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#save dataset\n",
    "#code adapted from https://stackoverflow.com/questions/3086973/how-do-i-convert-this-list-of-dictionaries-to-a-csv-file\n",
    "keys = dataset[0].keys()\n",
    "with open('stats.csv', 'w', newline='')  as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_utc = 31536000\n",
    "\n",
    "def generate_data_entry(text_instance, current_utc):\n",
    "    '''\n",
    "    Generate an entry to be added to the final dataset. Entry will be created from text_instance object. Text_instance can be either a submision or comment object.\n",
    "    \n",
    "    Returns dictionary(?) containing redditor name, age, gender, and punctuation statistics\n",
    "    Returns None if no useful data entry can be generated.\n",
    "    '''\n",
    "    # Check if author's account still exists\n",
    "    if text_instance.author is None:\n",
    "        return None\n",
    "    \n",
    "    text = None\n",
    "    if type(text_instance) is praw.models.Submission:\n",
    "        text = text_instance.title + text_instance.selftext\n",
    "    elif type(text_instance) is praw.models.reddit.comment.Comment:\n",
    "        text = text_instance.body\n",
    "    else:\n",
    "        #error\n",
    "        print(\"Item cannot be parsed (Is not a comment or submission)\")\n",
    "        print(type(text_instance))\n",
    "\n",
    "    #Find demographics\n",
    "    demog = find_demographics(text)\n",
    "    \n",
    "    if demog is None:\n",
    "        return None\n",
    "    \n",
    "    #Calculate current age\n",
    "    if demog[\"age\"] is not None:\n",
    "        utc_at_comment = text_instance.created_utc\n",
    "        years_since = int((current_utc - utc_at_comment)/year_utc)\n",
    "        demog[\"age\"] = demog[\"age\"] + years_since\n",
    "    \n",
    "    # For testing, make each entity row identifiable\n",
    "    demog[\"url\"] = text_instance.permalink\n",
    "    \n",
    "    #If demographics are found, calculate puncuation statistics\n",
    "    p_stats = calculate_punctuation_stats(text_instance.author,1596240000,1604188800)  #08/01/2020-11/01/2020\n",
    "    \n",
    "    if p_stats is None:\n",
    "        return None\n",
    "    \n",
    "    #Format and return data entry\n",
    "    return {**demog, **p_stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_demographics(text):\n",
    "    '''\n",
    "    Find the age and gender information contained in the text parameter.\n",
    "    \n",
    "    Returns a dictionary of the age and gender information contained in the text. If a feature cannot be determined, its dictionary value will be None.\n",
    "    '''\n",
    "    out = {}\n",
    "    out[\"age\"] = None\n",
    "    out[\"gender\"] = None\n",
    "    \n",
    "    ### find demographic info\n",
    "    # Check for ##Gender format with or without space or coma (Ex. 32M or 28 F or 23,M or 54, F)\n",
    "    match = re.findall(r\"\\b(\\d{2}),? ?\\/?(M|m|Male|male|F|f|Female|female)\\b\",text)\n",
    "    if len(match) == 1:\n",
    "        out[\"age\"] = int(match[0][0])\n",
    "        g = match[0][1]\n",
    "        if g in \"F f Female female\":\n",
    "            out[\"gender\"] = \"Female\"\n",
    "        else:\n",
    "            out[\"gender\"] = \"Male\"\n",
    "        return out\n",
    "    elif len(match) > 1:  # Catches cases where two people are identified (Ex. My wife (23f) and I (25M))\n",
    "        # first, focus matched text to more specific criteria including identifying pronoun\n",
    "        match = re.findall(r\"(?:I|i|me|Me|my|My|I'[a-zA-Z]+|i'[a-zA-Z]+)\\s?[\\(\\[\\s](\\d{2})[,\\/ ]?(M|m|Male|male|F|f|Female|female)[\\)\\]\\s]\",text)\n",
    "        if match:\n",
    "            out[\"age\"] = int(match[0][0])\n",
    "            g = match[0][1]\n",
    "            if g in \"F f Female female\":\n",
    "                out[\"gender\"] = \"Female\"\n",
    "            else:\n",
    "                out[\"gender\"] = \"Male\"\n",
    "            return out\n",
    "    \n",
    "    # Check for ## age format (Ex. 32 or 50)\n",
    "    match = re.findall(r\"\\b(\\d{2})[,.:-]?\\b\",text)\n",
    "    if match:\n",
    "        out[\"age\"] = int(match[0])\n",
    "        return out\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "import math\n",
    "\n",
    "def calculate_punctuation_stats(redditor, start_time, end_time):\n",
    "    '''\n",
    "    Generate punctuation statistics from posts of redditor during the time range between start_time and end_time (both in UTC, Unix time).\n",
    "    \n",
    "    Returns dictionary(?) of punctuation statistics\n",
    "    '''\n",
    "    comment_list = []  # contains strings of comment text\n",
    "    \n",
    "    try:\n",
    "        for comment in redditor.comments.new(limit=None):\n",
    "            if comment.created_utc > start_time and comment.created_utc < end_time:  # Check if comment is within time range\n",
    "                comment_list.append(comment.body)\n",
    "    except:\n",
    "        print(\"Account not valid (probably suspended).\")\n",
    "        \n",
    "    if len(comment_list) == 0:\n",
    "        return None\n",
    "        \n",
    "    # This function deals with reformatting of the comment-array input.\n",
    "\n",
    "    # Concatinates all the comments into one sample of the Redditor's writing (string object).\n",
    "    writing_sample = ' '.join(comment_list)\n",
    "    \n",
    "    # Remove new line\n",
    "    writing_sample = writing_sample.replace('\\n', \" \")\n",
    "\n",
    "    # Removing all non-ASCII letters\n",
    "    writing_sample = writing_sample.encode(\"ascii\", \"ignore\").decode()\n",
    "    \n",
    "    # Remove URLs from comments\n",
    "    writing_sample = re.sub(r'\\(https?:\\/\\/.*?\\)', '', writing_sample)\n",
    "\n",
    "    # Now, we need to check for instances of elipses and assign them to a new symbol--less confusion with periods, easier analysis.\n",
    "    # The tilde will represent an elipse from here on out.\n",
    "    writing_sample = writing_sample.replace('...', '~')\n",
    "\n",
    "    # We will loop over every character in the string and extract the punctuation, appending each mark to a list.\n",
    "    # We will keep this punctuation sequence in both list and string form for further analysis.\n",
    "    marks = ['.',',',':',';','?','!','-','(',')','\"',\"'\",'~']\n",
    "    marks_str = ''.join(map(str, marks))\n",
    "\n",
    "    punct_list = []\n",
    "\n",
    "    for character in writing_sample:\n",
    "        if character in marks:\n",
    "            punct_list.append(character)\n",
    "\n",
    "    punct_str = ''.join(punct_list)\n",
    "    \n",
    "    if len(punct_list) == 0:\n",
    "        return None\n",
    "\n",
    "    # Now, we want to get our input in the following format..[2, '!', 3, '.'].\n",
    "    split_by_punc = (x.split() for x in re.split('[' + marks_str + ']', writing_sample))\n",
    "    wrds_btwn = list(map(len, split_by_punc))\n",
    "    woven = list(chain.from_iterable(zip(wrds_btwn, punct_list)))\n",
    "    \n",
    "    # Develop punctuation stats for comments_text\n",
    "    # Generation of F1, relative frequency of each punctuation mark of interest.\n",
    "    num_period = punct_str.count('.')\n",
    "    num_comma = punct_str.count(',')\n",
    "    num_colon = punct_str.count(':')\n",
    "    num_semicol = punct_str.count(';')\n",
    "    num_question = punct_str.count('?')\n",
    "    num_exclaim = punct_str.count('!')\n",
    "    num_dash = punct_str.count('-')\n",
    "    num_left_paren = punct_str.count('(')\n",
    "    num_right_paren = punct_str.count(')')\n",
    "    num_sing_quote = punct_str.count(\"'\")\n",
    "    num_doub_quote = punct_str.count('\"')\n",
    "    num_elipse = punct_str.count('~')\n",
    "\n",
    "    mark_counts = [num_period, num_comma, num_colon, num_semicol, num_question, num_exclaim, num_dash, num_left_paren, num_right_paren, num_sing_quote, num_doub_quote, num_elipse] \n",
    "    F1 = [x / len(punct_list) for x in mark_counts]\n",
    "    \n",
    "    # Generation of F2 and F3, conditional/joint probability of observing two punctuation marks in succession.\n",
    "    F2_dict = {}\n",
    "    F3_dict = {}\n",
    "    successive_pairs = []\n",
    "    \n",
    "    combos = list(itertools.product(marks_str, marks_str))\n",
    "    indices = list(range(len(marks_str)))\n",
    "    index_combos = list((itertools.product(indices, indices)))\n",
    "    index_strings = [[str(x) for x in tup] for tup in index_combos]\n",
    "    final_ind = list(map(' '.join, data)) \n",
    "    #combos = list(itertools.product(marks_str, marks_str))\n",
    "    #indices = list(range(marks_str)) \n",
    "    #index_combos = list(itertools.product(indices, indices)) \n",
    "\n",
    "    for i in combos:\n",
    "        successive_pairs.append(''.join(i))\n",
    "        \n",
    "    zipped = list(zip(final_ind, successive_pairs)) \n",
    "\n",
    "    for (index, pair) in zipped:\n",
    "        numerator = punct_str.count(pair)\n",
    "        denominator = punct_str.count(pair[0])\n",
    "\n",
    "        if numerator != 0:\n",
    "            F2_dict['Conditional Prob' + ' ' + index + ' ' + pair + ' '] = (numerator/denominator)\n",
    "            F3_dict['Joint Prob' + ' ' + index + ' ' + pair + ' '] = (numerator/len(punct_list))\n",
    "        else:\n",
    "            F2_dict['Conditional Prob' + ' ' + index + ' ' + pair + ' '] = 0\n",
    "            F3_dict['Joint Prob' + ' ' + index + ' ' + pair + ' '] = 0\n",
    "            \n",
    "    # Generation of f4, redditor's average sentence length in words.\n",
    "    sent_lens = []\n",
    "    max_sents = 200\n",
    "    regex = \"\\w+('\\w+)?(?<!('s))\"\n",
    "    sentences = split_into_sentences(writing_sample)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sent_lens.append(len(re.findall(regex, sentence)))\n",
    "\n",
    "    if len(sent_lens) == 0:\n",
    "        sent_lens.append(0)\n",
    "    \n",
    "    f4 = mean(sent_lens)\n",
    "\n",
    "    # f4 as a probability distribution of sentence length--as it is in Darmon's work.\n",
    "    # Cap sentence length at 199 words.\n",
    "    F4 = [0]*max_sents\n",
    "    \n",
    "    sent_lens = [x if x < max_sents else max_sents-1 for x in sent_lens]\n",
    "\n",
    "    for i in sent_lens:\n",
    "        F4[i] += 1\n",
    "\n",
    "    F4 = [x / len(sent_lens) for x in F4]\n",
    "    \n",
    "    # Generation of f5, redditor's average number of words between successive punctuation marks.\n",
    "    if len(wrds_btwn) == 0:\n",
    "        wrds_btwn.append(0)\n",
    "        \n",
    "    f5 = mean(wrds_btwn)\n",
    "\n",
    "    # f5 as a probability distribution of number of words between successive marks--as it is in Darmon's work.\n",
    "    # Cap number of in-between words at 39.\n",
    "    max_wrds_btwn = 40\n",
    "    wrds_btwn = [x if x < max_wrds_btwn else max_wrds_btwn-1 for x in wrds_btwn]\n",
    "    F5 = [0]*max_wrds_btwn\n",
    "\n",
    "    for i in wrds_btwn:\n",
    "        F5[i] += 1\n",
    "\n",
    "    F5 = [x / len(wrds_btwn) for x in F5]\n",
    "    \n",
    "    # Generation of f6, ratio of punctuation to words.\n",
    "    total_punc = len(punct_list)\n",
    "    total_words = len(re.findall(regex, writing_sample))\n",
    "\n",
    "    f6 = total_punc/total_words\n",
    "    \n",
    "    # Output to dataframe.\n",
    "    # An uppercase F signals that the feature was mimicked directly from Darmon's work.\n",
    "    F1_dict_keys = ['Period Freq', 'Comma Freq', 'Colon Freq', 'Semicol Freq', 'Question Freq', 'Exclaim Freq', 'Dash Freq', 'L. Paren Freq', 'R. Paren Freq', 'S. Quote Freq', 'D. Quote Freq', 'Elipse Freq']\n",
    "    F1_dict = {F1_dict_keys[i]: F1[i] for i in range(len(F1_dict_keys))} \n",
    "\n",
    "    F4_dict = {'Prob Sentence Len:' + ' ' + str(i): F4[i] for i in range(len(F4))}\n",
    "    F5_dict = {'Prob' + ' ' + str(i) + ' ' + 'Words Between Punc': F5[i] for i in range(len(F5))}\n",
    "\n",
    "    f4_dict = {'Avg Sentence Len': f4}\n",
    "    f5_dict = {'Avg Wrds Btwn Punc': f5}\n",
    "    f6_dict = {'Punc Ratio': f6}\n",
    "    \n",
    "    # Concatinate all the keys into a single list.\n",
    "    master = {**F1_dict, **F2_dict, **F3_dict, **F4_dict, **F5_dict, **f4_dict, **f5_dict, **f6_dict}\n",
    "    \n",
    "    return master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function which splits text input into individual sentances. Used in generation of f4.\n",
    "\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "alphabets= \"([A-Za-z])\"\n",
    "digits = \"([0-9])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text) \n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
